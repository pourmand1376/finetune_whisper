{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12a576f-5aef-4671-8a6b-b1c7d0cff20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cb1322-c237-4d6a-8c76-679a795f14e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe74aacfd294dcc9aa226db7a1e9403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a7c8f4bfda40d39dd101213a5d2365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice=load_dataset('common_voice_processed.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18becbc4-b392-4cfc-b446-322b202966d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 39357\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 10547\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e001c81-5fe3-4ade-81d7-29781f8af8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f3eaa3-212e-4dd4-aeff-8bb4f254ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"Persian\", task=\"transcribe\",cache_dir='v3')\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308880a7-489f-420d-b35b-b1bf452ea32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e832e016-9cff-4c7c-b422-d2e2cba1b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5fbea83-08fb-4f47-8ef1-b0fd952ffbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\",cache_dir='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff49974d-3750-416a-b43d-82e8951e7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a66654c-a195-464a-8798-78449aa2a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-medium-fa\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7d1a84-9818-4e1c-bb4f-d467764729c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf0bc32-bb1d-4fd9-87d1-7491cddb3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ade94-7a24-46ee-804a-b4d21ff47308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# multigpu\n",
    "accelerator = Accelerator()\n",
    "my_trainer=accelerator.prepare(trainer)\n",
    "my_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24e4d8a-7928-4a5c-9830-7ab68a50c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3315171-6797-47b8-a135-de4ad2bd0c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mAccelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice_placement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmixed_precision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PrecisionType | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdeepspeed_plugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DeepSpeedPlugin | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfsdp_plugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FullyShardedDataParallelPlugin | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmegatron_lm_plugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'MegatronLMPlugin | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrng_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str | RNGType] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog_with\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | LoggerType | GeneralTracker | list[str | LoggerType | GeneralTracker] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproject_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | os.PathLike | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproject_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ProjectConfiguration | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgradient_accumulation_plugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'GradientAccumulationPlugin | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdispatch_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meven_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_seedable_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstep_scheduler_with_optimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkwargs_handlers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[KwargsHandler] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdynamo_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DynamoBackend | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Creates an instance of an accelerator for distributed training (on multi-GPU, TPU) or mixed precision training.\n",
       "\n",
       "Args:\n",
       "    device_placement (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not the accelerator should put objects on device (tensors yielded by the dataloader, model,\n",
       "        etc...).\n",
       "    split_batches (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
       "        `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
       "        round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
       "        in your script multiplied by the number of processes.\n",
       "    mixed_precision (`str`, *optional*):\n",
       "        Whether or not to use mixed precision training. Choose from 'no','fp16','bf16 or 'fp8'. Will default to the\n",
       "        value in the environment variable `ACCELERATE_MIXED_PRECISION`, which will use the default value in the\n",
       "        accelerate config of the current system or the flag passed with the `accelerate.launch` command. 'fp8'\n",
       "        requires the installation of transformers-engine.\n",
       "    gradient_accumulation_steps (`int`, *optional*, default to 1):\n",
       "        The number of steps that should pass before gradients are accumulated. A number > 1 should be combined with\n",
       "        `Accelerator.accumulate`. If not passed, will default to the value in the environment variable\n",
       "        `ACCELERATE_GRADIENT_ACCUMULATION_STEPS`. Can also be configured through a `GradientAccumulationPlugin`.\n",
       "    cpu (`bool`, *optional*):\n",
       "        Whether or not to force the script to execute on CPU. Will ignore GPU available if set to `True` and force\n",
       "        the execution on one process only.\n",
       "    deepspeed_plugin (`DeepSpeedPlugin`, *optional*):\n",
       "        Tweak your DeepSpeed related args using this argument. This argument is optional and can be configured\n",
       "        directly using *accelerate config*\n",
       "    fsdp_plugin (`FullyShardedDataParallelPlugin`, *optional*):\n",
       "        Tweak your FSDP related args using this argument. This argument is optional and can be configured directly\n",
       "        using *accelerate config*\n",
       "    megatron_lm_plugin (`MegatronLMPlugin`, *optional*):\n",
       "        Tweak your MegatronLM related args using this argument. This argument is optional and can be configured\n",
       "        directly using *accelerate config*\n",
       "    rng_types (list of `str` or [`~utils.RNGType`]):\n",
       "        The list of random number generators to synchronize at the beginning of each iteration in your prepared\n",
       "        dataloaders. Should be one or several of:\n",
       "\n",
       "        - `\"torch\"`: the base torch random number generator\n",
       "        - `\"cuda\"`: the CUDA random number generator (GPU only)\n",
       "        - `\"xla\"`: the XLA random number generator (TPU only)\n",
       "        - `\"generator\"`: the `torch.Generator` of the sampler (or batch sampler if there is no sampler in your\n",
       "          dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.\n",
       "\n",
       "        Will default to `[\"torch\"]` for PyTorch versions <=1.5.1 and `[\"generator\"]` for PyTorch versions >= 1.6.\n",
       "    log_with (list of `str`, [`~utils.LoggerType`] or [`~tracking.GeneralTracker`], *optional*):\n",
       "        A list of loggers to be setup for experiment tracking. Should be one or several of:\n",
       "\n",
       "        - `\"all\"`\n",
       "        - `\"tensorboard\"`\n",
       "        - `\"wandb\"`\n",
       "        - `\"comet_ml\"`\n",
       "        If `\"all\"` is selected, will pick up all available trackers in the environment and initialize them. Can\n",
       "        also accept implementations of `GeneralTracker` for custom trackers, and can be combined with `\"all\"`.\n",
       "    project_config (`ProjectConfiguration`, *optional*):\n",
       "        A configuration for how saving the state can be handled.\n",
       "    project_dir (`str`, `os.PathLike`, *optional*):\n",
       "        A path to a directory for storing data such as logs of locally-compatible loggers and potentially saved\n",
       "        checkpoints.\n",
       "    dispatch_batches (`bool`, *optional*):\n",
       "        If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
       "        and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
       "        underlying dataset is an `IterableDataset`, `False` otherwise.\n",
       "    even_batches (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
       "        dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
       "        all workers.\n",
       "    use_seedable_sampler (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not use a fully seedable random sampler ([`~data_loader.SeedableRandomSampler`]). Ensures\n",
       "        training results are fully reproducable using a different sampling technique. While seed-to-seed results\n",
       "        may differ, on average the differences are neglible when using multiple different seeds to compare. Should\n",
       "        also be ran with [`~utils.set_seed`] for the best results.\n",
       "    step_scheduler_with_optimizer (`bool`, *optional`, defaults to `True`):\n",
       "        Set `True` if the learning rate scheduler is stepped at the same time as the optimizer, `False` if only\n",
       "        done under certain circumstances (at the end of each epoch, for instance).\n",
       "    kwargs_handlers (`list[KwargHandler]`, *optional*)\n",
       "        A list of `KwargHandler` to customize how the objects related to distributed training or mixed precision\n",
       "        are created. See [kwargs](kwargs) for more information.\n",
       "    dynamo_backend (`str` or `DynamoBackend`, *optional*, defaults to `\"no\"`):\n",
       "        Set to one of the possible dynamo backends to optimize your training with torch dynamo.\n",
       "    gradient_accumulation_plugin (`GradientAccumulationPlugin`, *optional*):\n",
       "        A configuration for how gradient accumulation should be handled, if more tweaking than just the\n",
       "        `gradient_accumulation_steps` is needed.\n",
       "\n",
       "**Available attributes:**\n",
       "\n",
       "    - **device** (`torch.device`) -- The device to use.\n",
       "    - **distributed_type** ([`~utils.DistributedType`]) -- The distributed training configuration.\n",
       "    - **local_process_index** (`int`) -- The process index on the current machine.\n",
       "    - **mixed_precision** (`str`) -- The configured mixed precision mode.\n",
       "    - **num_processes** (`int`) -- The total number of processes used for training.\n",
       "    - **optimizer_step_was_skipped** (`bool`) -- Whether or not the optimizer update was skipped (because of\n",
       "      gradient overflow in mixed precision), in which\n",
       "    case the learning rate should not be changed.\n",
       "    - **process_index** (`int`) -- The overall index of the current process among all processes.\n",
       "    - **state** ([`~state.AcceleratorState`]) -- The distributed setup state.\n",
       "    - **sync_gradients** (`bool`) -- Whether the gradients are currently being synced across all processes.\n",
       "    - **use_distributed** (`bool`) -- Whether the current configuration is for distributed training.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/accelerate/accelerator.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "Accelerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73667b9d-00ed-4424-8307-600584c0de91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
